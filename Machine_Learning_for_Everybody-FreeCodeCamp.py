# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Miyac1mt8HY7VNWbg3jiA4RUumozl-UY
"""

import tensorflow as tf

def train_model(
    X_train, y_train,
    hidden_units=(64, 32),         # number of neuron per layer
    dropout_prob=0.2,              # percentage of random neuron that will be inactive, so model does not rely too heavily on a neuron
    learning_rate=1e-3,
    batch_size=32,                 # number of samples befor weight is updated
    epochs=50,                     # number of times we will use the whole dataset
    validation_split=0.2,          # measure performance, overfitting, checks after epoch,
    use_early_stopping=True
    ):

    n_features = X_train.shape[1]

    # Build model
    layers = [tf.keras.layers.Input(shape=(n_features,))]
    for units in hidden_units:
        layers.append(tf.keras.layers.Dense(units, activation='relu'))
        if dropout_prob and dropout_prob > 0:
            layers.append(tf.keras.layers.Dropout(dropout_prob))
    layers.append(tf.keras.layers.Dense(1, activation='sigmoid'))
    nn_model = tf.keras.Sequential(layers)

    # Configuration of the model (setup)
    nn_model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                 tf.keras.metrics.AUC(name='auc')]
        )

    # Callbacks
    callbacks = []                 # stores the best model
    if use_early_stopping:
        callbacks.append(tf.keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=5, restore_best_weights=True
        ))                         # patience is number of epochs with no improvement after which training will be stopped
                                   # monitors value, default- "val_loss"

    #actual training loop
    history = nn_model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=validation_split,
        callbacks=callbacks,
        verbose=1
    )#returns a History object, history.history is a dict that stores per-epoch metric lists
    return nn_model, history

model.compile(
    optimizer="adam",                  # learning rate
    loss="binary_crossentropy",        # loss function
    metrics=["accuracy"],              # list ofmeasurement that are tracked
    loss_weights=None,                 # for multi-output tells model how to treat loss for different outputs
    weighted_metrics=None,             # treats different metrics differently
    run_eagerly=None,                  # force eager mode for debugging
    steps_per_execution=None           # processes multiple batches at once
)


model.fit(
    x, y=None,                         # training data (numpy, tf.data, generatorâ€¦)
    batch_size=None,                   # number of samples per gradient update (no. of many parts the dataset is divided)
    epochs=1,                          # how many passes through dataset (no. of times model is trained)
    verbose=1,                         # 0,1,2 (progress bar per epoch showing metrics (loss, val_loss, accuracy, etc.), time)
    callbacks=None,                    # list of callback objects
    validation_split=0.0,              # % of training data used for validation (0-1)
    validation_data=None,              # (x_val, y_val) overrides validation_split
    shuffle=True,                      # shuffle training data befor each epoch
    class_weight=None,                 # dictionary, adjust loss importance for class imbalance
    sample_weight=None,                # list, adjusts loss importance for individual sample
    initial_epoch=0,                   # resuming training from the nth epoch
    steps_per_epoch=None,              # number of epochs in training for generator or unknow dataset size
    validation_steps=None,             # number of epochs in validation for generator or unknow dataset size
    validation_batch_size=None,        # batch size for validation
    validation_freq=1,                 # how often to run validation
    max_queue_size=10,                 # number of batchs keept ready in queue for generator queues
    workers=1,                         # number of parallel processes for loading data
    use_multiprocessing=False          # multiprocessing for loading data
)


tf.keras.callbacks.                    # Effects
ModelCheckpoint()	                     # Saves weights/best model during training
EarlyStopping()	                       # Stops training early if no improvement
ReduceLROnPlateau()	                   # Lowers LR automatically when metric plateaus(loss value stops decreasing)
LearningRateScheduler()	               # Custom per-epoch LR schedule
TensorBoard()	                         # Logs metrics for visualization
CSVLogger()	                           # Saves training history into CSV file
ProgbarLogger()	                       # Prints progress bar (default)
LambdaCallback()	                     # Fully custom actions at batch/epoch start/end
